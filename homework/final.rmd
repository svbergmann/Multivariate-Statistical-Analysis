---
title: "Multivariate Statistical Analysis"
subtitle: "Final work"
author: Lucas Fellmeth, Helen Kafka, Sven Bergmann
date: 05/09/24
output: pdf_document
---

```{r, echo = F}
library(knitr)
opts_chunk$set(tidy.opts = list(width.cutoff = 60), tidy = TRUE)
```
```{r, warning = F, message = F, error = F}
library(nnet)
library(caret)
library(kernlab)
```
# Problem 1

Consider the ``vehicle`` dataset.
The goal is to identify 3D objects from 2D images captured by cameras at different angles.
The objects in this case are four types of vehicles
(identified by the variables ``class`` and ``classdigit``),
and the other 18 numerical variables are measurements extracted from these 2D images.

```{r}
vehicles <- read.csv(file = './Data_csv/vehicle.csv')
# remove the classdigit because we already have a class variable
vehicles <- subset(vehicles, select = -classdigit)
vehicles$class <- factor(vehicles$class)
```

Split the data into training and test sets (80/20).

```{r}
set.seed(42)
i_test_vehicles <- sample(seq_along(vehicles[, 1]), size = length(vehicles[, 1]) * 0.2)
vehicles_test <- vehicles[i_test_vehicles,]
vehicles_train <- vehicles[-i_test_vehicles,]
```

On the training set compute:

- a multinomial logistic classifier

```{r}
vehicles_mlc <- glm(formula = class ~ . - class, data = vehicles_train, family = binomial)
summary(vehicles_mlc)
```

- a single-hidden-layer neural network classifier (with the number of hidden nodes to be determined)

```{r}
vehicles_num_hidden_nodes <- 20
vehicles_nnets <- vector(mode = "list", length = vehicles_num_hidden_nodes)
for (i in 1:vehicles_num_hidden_nodes) {
  vehicles_nnets[[i]] <- nnet(class ~ . - class, data = vehicles_train, size = i, trace = F)
}
```

On the test data,
find the cross-classification tables and the misclassification rates.

```{r}
vehicles_mlc_pi1_hat <- predict(vehicles_mlc, vehicles_test, type = 'response')
vehicles_mlc_gr_hat <- ifelse(vehicles_mlc_pi1_hat > 0.5, 2, 1)
vehicles_mlc_mctable <- table(vehicles_mlc_gr_hat, vehicles_test$class)
vehicles_mlc_mctable
```

```{r}
1 - sum(diag(vehicles_mlc_mctable)) / length(vehicles_test$class)
```

```{r}
vehicles_nnets_cms <- vector(mode = 'list', length = vehicles_num_hidden_nodes)
vehicles_nnets_mscrs <- list()
for (i in 1:vehicles_num_hidden_nodes) {
  vehicles_nnets_cms[[i]] <- confusionMatrix(
    factor(
      predict(vehicles_nnets[[i]], newdata = vehicles_test, type = 'class'),
      levels = levels(vehicles_test$class)),
    vehicles_test$class)
  vehicles_nnets_mscrs[i] <- 1 - vehicles_nnets_cms[[i]]$overall["Accuracy"]
}
```

```{r}
for (i in 1:vehicles_num_hidden_nodes) {
  cat('Misclassification rate for', i, 'hidden nodes:', vehicles_nnets_mscrs[[i]], '\n')
}
```

```{r}
vehicles_nnets_min_mscrs <- which.min(vehicles_nnets_mscrs)
cat('The lowest misclassificationrate is on the neural network \n with', vehicles_nnets_min_mscrs, 'hidden nodes and a rate of', vehicles_nnets_mscrs[[min_mscrs_vehicles]], '.')
```

```{r}
vehicles_nnets_cms[[min_mscrs_vehicles]]
```

Which of the above methods is better?
Is there any specific type of vehicle that is harder to classify than the others?


# Problem 2

The ``pendigitss`` dataset consists of discretized handwritten digits
(for a full description, see Section 7.2.10 in the book).
From this set, extract the subset corresponding to digits 0, 6, 8 and 9,
and scale the variables so that the variances are 1.

```{r}
pendigits <- read.csv(file = './Data_csv/pendigits.csv')
pendigits$digit <- factor(pendigits$digit)
pendigits <- pendigits[pendigits$digit %in% c(0, 6, 8, 9),]
pendigits[, 2:17] <- scale(pendigits[, 2:17])
```

## (a)
Compute ordinary principal components and draw a scatterplot of the first two component scores,
using different colors (or symbols) for different digits.
Are the digits well separated?

```{r}
pca_pendigits <- princomp(pendigits[, 2:ncol(pendigits)])
plot(pca_pendigits$scores[, 1] ~ pca_pendigits$scores[, 2], col = pendigits$digit, pch = 16)
legend('topleft', col = 1:10, legend = paste('Number', 1:10), pch = 16, bty = 'n')
```

## (b)
Compute kernel principal components using Gaussian kernels with various scales,
and draw scatterplots of the first two component scores as in (a).

```{r}
kpca_pendigits <- kpca(~. - digit, data = pendigits, kernel = 'rbfdot', kpar = list(sigma = 1), features = 2)
plot(rotated(kpca_pendigits)[, 1], rotated(kpca_pendigits)[, 2], pch = 16, col = as.numeric(pendigits$digit))
legend('topleft', col = 1:10, legend = paste('Number', 1:10), pch = 16, bty = 'n')
```

Are the digits now better separated than in (a)?

